---
title: "SFB 980"
author: Till Grallert
date: 2017-05-14 21:06:36 +0200
---

# Rahmen

- Wann: 1. Juni
- Wo: Berlin
- Worum geht es, was wollen die von uns?
    + Torsten war beim DH Arbeitskreis
    + Bestände sind schlecht erschlossen
- SFB 980: Episteme in Bewegung

# Abstract
## Arabische Buch-, Sammlungs- und Rezeptionsgeschichte: Zwei Werkstattberichte

Anhand von zwei Werkstattberichten zur Frühen Neuzeit und dem ausgehenden 19. Jahrhundert führt der Vortrag in epistemologische und praktische Problematiken der Arbeit mit digitalen arabischen Texten ein. 
Zunächst geht der Bericht zu einer frühneuzeitlichen Autorenbibliothek stärker auf epistemologische und methodische Fragen der Buch- und Sammlungsgeschichte ein. Das Werk des Autors zeichnet sich insbesondere durch einen großen Anteil an Sammelhandschriften aus, bei denen das Buch als textliche und als physische Einheit auseinanderfallen. 
Das daraufhin vorgestellte Projekt OpenArabicPE untersucht zum einen die Rezeptionsgeschichte von Bucherscheinungen in zwei Damaszener Zeitschriften und zum anderen die Autorennetzwerke, die sich in beiden Zeitschriften nachverfolgen lassen. In diesem Kontext werden Ansätze zum Auszeichnen relevanter Informationen (z.B. mit TEI) und zur Verlinkung mit Authority Files (z.B. OCLC, VIAF) vorgestellt und die epistemische Gewalt von Werkzeugen problematisiert.

# Torstens vier Punkte

1. Wie gut funktioniert OCR auf Arabisch? Nicht so gut
2. Wie ist der Stand der Erfassung historischer Sammlungen arabischer Texte? Nicht so gut
3. Was kann man dagegen tun? Vorstellung deiner Edition und vielleicht von Maxims Projekt, dass tausende Texte an einem Ort durchsuchbar macht.
4. Wie wurschtelt man drum herum? Markup, Tagging, etc.

# Ideen

- Probleme im Feld
    + OCR -> keine Volltexteditionen
    + Keine zentralen Findmittel
    + Unsere Werkzeuge stammen aus dem westlichen Epistem:
        * Konzepte von Text, Namen, Daten
- Mögliche Ansätze
    + Mis-en-page: i.e. gute Bildbeschreibungen, vielleicht IIIF etc.
    + Bootstrapping everything: OpenArabicPE
    + Corpus linguistics: OpenArabic
- Erfahrungsberichte
    + praktisches Arbeiten an digitalen Editionen
    + praktisches Arbeiten


# Verfolung von intellektuellen Netzwerken

- Was wurde in Damaskus gelesen?
- Wer hat in Damaskus veröffentlicht?
- Wichtigkeit von authority files
- Wichtigkeit von LOD
    + Problem von LOD

## notes

- Having a digital text as a string of characters does not solve all issues
- modelling and mark-up as necessary analytical steps
    + available mark-up schemes despite being language agnostic are rooted in the Western episteme
- linguistic analysis
    + Arabic NLP is progressing but I am not familiar with the necessary tools and techniques
    + Arabic NER is not readily available
- TEI (structural / semantic mark-up):
    + modells entities according to our Western perception of names
- XML:
    + is officially language agnostic (within the limits of unicode), but rarely have I seen Arabic tags
    + XPath specification for non-Gregorian calendars is buggy
- Disambiguation:
    + Unlike Western languages, Arabic tends not to conflate toponyms and personal names (i.e. the infamous Paris case)
    + Disambiguation is needed for names referring to more than one entity
- Authority files
    + One possibility to disambiguate with reference to a unique ID within an authority file 
    + External authority files and LOD provide additional information on identified entities (dates, further names, locations, lists of works etc. )
        * VIAF
        * GND
        * GeoNames
    + Problems: 
        1. many Arabic authors, works, places are not recorded in any authority file
        2. authority files will return false positives:
            - GeoNames does not identify "باريز" as the French capital Paris
            - GeoNames does not identify "فينا" as the Vienna but a town in South Africa
    + Potential / proposed solution:
        1. canonical references
        there is many biographical dictionaries, publication catalogues that could be used either through referencing well-known editions or the number of an entry


### geoJSON

1. Formal specification
2. GitHub automatically renders geoJSON with the help of Mapbox.js: [tutorial](https://help.github.com/articles/mapping-geojson-files-on-github/)
    - these maps can be embedded in websites: `<script src="https://embed.github.com/view/geojson/<username>/<repo>/<ref>/<path_to_file>"></script>`, e.g.

    `<script src="https://embed.github.com/view/geojson/OpenArabicPE/slides/gh-pages/assets/maps/muqtabas_bylines-toponyms.geojson?height=300&width=800"></script>``

4. mapbox.js
    - [toturial for markers](https://www.mapbox.com/help/markers/)
    - [csv to geojson](https://mapbox.github.io/csv2geojson/)
3. Websites such as [geojson.io](http://geojson.io/) render and generate geoJSON
    - <!-- [geojson.io](http://geojson.io/) --> can also be used for geo-reference toponyms
    - <!-- [geojson.io](http://geojson.io/) --> provides export as geoJSON, KML, topoJSON, CSV etc. 
4. The popular platform Carto (formerly CartoDB) can injest geoJSON straight from GitHub ([example](https://tillgrallert.carto.com/builder/4d63ebfc-3f0c-11e7-8dc7-0ef24382571b/embed?state=%7B%22map%22%3A%7B%22ne%22%3A%5B-68.9110048456202%2C-71.36718750000001%5D%2C%22sw%22%3A%5B79.30263962053661%2C87.5390625%5D%2C%22center%22%3A%5B19.31114335506464%2C8.085937500000002%5D%2C%22zoom%22%3A3%7D%2C%22widgets%22%3A%7B%22b5cdac1f-ce78-4f9e-9645-f03df135e084%22%3A%7B%22autoStyle%22%3Atrue%7D%7D%7D)).
    - it seems impossible to delete / remove data from Carto

### canonical references: 

- make-shift canonical references could follow this pattern:
    1. oclc numbers identified concrete manifestations of a work (i.e. a specific edition): `oclc_124563`
    2. page / column number: `pb_123` and/or `cb_1`




## methodology

1. Mark up bylines (`<byline>`) and bibliographic references (`<bibl>`)
2. Try to semi-automatically link every author name to an authority file (e.g. local file, VIAF)
    - XSLT: link to existing entries in local authority files
    - OpenRefine: "reconcile" remaining data with external authority files
2. Generate MODS xml files ["**a**"] for all articles in *al-Muqtabas* / *al-Ḥaqāʾiq*
3. Generate TEI xml with all bylines
    - this can be used for geographic analysis
    1. run `tei_extract-bylines-stub.xsl` on every TEI file of the edition
    2. merge the resulting files with terminal `$cat`: saved as `oclc_4770057679-bylines.TEIP5.xml` for *al-Muqtabas*
    3. run `tei_group-bylines-by-place.xsl` on the result, which will generate a CSV of toponyms, number of articles, GeoNames ID, coordinates
    4. this can be converted to [geoJSON](http://geojson.org/) (which allows for immediate mapping with GitHub or [geojson.io](http://geojson.io/))
        + for examples see [here](https://github.com/OpenArabicPE/slides/blob/gh-pages/assets/maps/muqtabas_reviews-toponyms.geojson)
3. Generate MODS xml files ["**b**"] for all bibliographic references in *al-Muqtabas* / *al-Ḥaqāʾiq*
4. Generate statistics from **a** and **b**
    1. authors
        - articles / works per author (total and per year)
        - age of the author upon publication
        - number of pages
    2. places of publication
        - articles / works per place
        - time difference between publication of a work and a reference in *al-Muqtabas* / *al-Ḥaqāʾiq*
6. Visualisation
